"""
æµ‹è¯• Tools ä¸ LLM çš„é›†æˆ
äº¤äº’å¼æµ‹è¯•è„šæœ¬
"""
import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from openai import AsyncOpenAI
from src.container import init_services
from src.config import settings
from tools import get_tools_for_llm, execute_tool_calls_from_message


async def chat_with_tools():
    """ä¸ LLM è¿›è¡Œå¸¦å·¥å…·çš„å¯¹è¯"""
    
    # åˆå§‹åŒ–æœåŠ¡
    print("ğŸ”§ åˆå§‹åŒ–æœåŠ¡...")
    init_services()
    
    # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ Qwen APIï¼‰
    client = AsyncOpenAI(
        api_key=settings.llm_api_key,
        base_url=settings.llm_base_url
    )
    
    # è·å–å·¥å…·åˆ—è¡¨
    tools = get_tools_for_llm()
    print(f"âœ… å·²åŠ è½½ {len(tools)} ä¸ªå·¥å…·\n")
    
    # ç³»ç»Ÿæç¤º
    system_prompt = """ä½ æ˜¯ä¸€ä¸ª ASD å„¿ç«¥å¹²é¢„åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥ï¼š
1. æŸ¥è¯¢å’Œç®¡ç†å­©å­çš„æ¡£æ¡ˆä¿¡æ¯ï¼ˆSQLiteï¼‰
2. ä¿å­˜å’ŒæŸ¥è¯¢å¹²é¢„è®°å¿†ï¼ˆMemoryï¼‰
3. åˆ†æå¹²é¢„è§†é¢‘ï¼ˆVideo Analysisï¼‰
4. æ£€ç´¢æ–¹æ³•è®ºã€æ¸¸æˆã€é‡è¡¨ç­‰çŸ¥è¯†ï¼ˆRAGï¼‰

è¯·æ ¹æ®ç”¨æˆ·çš„éœ€æ±‚ï¼Œé€‰æ‹©åˆé€‚çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚"""
    
    # å¯¹è¯å†å²
    messages = [
        {"role": "system", "content": system_prompt}
    ]
    
    print("=" * 60)
    print("ğŸ¤– ASD å¹²é¢„åŠ©æ‰‹ï¼ˆå¸¦å·¥å…·è°ƒç”¨ï¼‰")
    print("=" * 60)
    print("æç¤ºï¼šè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("æç¤ºï¼šè¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
    print("æç¤ºï¼šè¾“å…¥ 'tools' æŸ¥çœ‹å¯ç”¨å·¥å…·")
    print("=" * 60)
    print()
    
    while True:
        # è·å–ç”¨æˆ·è¾“å…¥
        try:
            user_input = input("ğŸ‘¤ ä½ : ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        
        if not user_input:
            continue
        
        # å¤„ç†ç‰¹æ®Šå‘½ä»¤
        if user_input.lower() in ['quit', 'exit']:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        
        if user_input.lower() == 'clear':
            messages = [{"role": "system", "content": system_prompt}]
            print("âœ… å¯¹è¯å†å²å·²æ¸…ç©º\n")
            continue
        
        if user_input.lower() == 'tools':
            print("\nğŸ“¦ å¯ç”¨å·¥å…·åˆ—è¡¨:")
            print("  SQLite: get_child_profile, save_child_profile, create_session, update_session, get_session_history, delete_child")
            print("  Memory: save_memories, get_recent_memories, build_context, analyze_trends, detect_milestones, clear_memories")
            print("  Video: analyze_video")
            print("  RAG: search_methodology, search_games, search_games_by_dimension, search_games_by_interest, get_game_details, search_scale, search_cases")
            print()
            continue
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        messages.append({"role": "user", "content": user_input})
        
        try:
            # ç¬¬ä¸€æ¬¡è°ƒç”¨ LLM
            print("ğŸ¤” æ€è€ƒä¸­...")
            response = await client.chat.completions.create(
                model=settings.llm_model,
                messages=messages,
                tools=tools,
                tool_choice="auto",
                temperature=0.7
            )
            
            message = response.choices[0].message
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
            if message.tool_calls:
                print(f"ğŸ”§ éœ€è¦è°ƒç”¨ {len(message.tool_calls)} ä¸ªå·¥å…·:")
                for tc in message.tool_calls:
                    print(f"  - {tc.function.name}")
                
                # å°† LLM çš„å“åº”æ·»åŠ åˆ°æ¶ˆæ¯å†å²
                messages.append({
                    "role": "assistant",
                    "content": message.content,
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": tc.type,
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments
                            }
                        }
                        for tc in message.tool_calls
                    ]
                })
                
                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                print("âš™ï¸  æ‰§è¡Œå·¥å…·...")
                tool_results = await execute_tool_calls_from_message(message)
                
                # æ˜¾ç¤ºå·¥å…·ç»“æœï¼ˆç®€åŒ–ï¼‰
                for result in tool_results:
                    content = result['content']
                    if len(content) > 100:
                        content = content[:100] + "..."
                    print(f"  âœ… {result['name']}: {content}")
                
                # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
                messages.extend(tool_results)
                
                # ç¬¬äºŒæ¬¡è°ƒç”¨ LLMï¼ˆè·å–æœ€ç»ˆç­”æ¡ˆï¼‰
                print("ğŸ’­ æ•´ç†ç­”æ¡ˆ...")
                final_response = await client.chat.completions.create(
                    model=settings.llm_model,
                    messages=messages,
                    temperature=0.7
                )
                
                final_message = final_response.choices[0].message
                assistant_reply = final_message.content
                
                # æ·»åŠ åˆ°å†å²
                messages.append({"role": "assistant", "content": assistant_reply})
            else:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥å›å¤
                assistant_reply = message.content
                messages.append({"role": "assistant", "content": assistant_reply})
            
            # æ˜¾ç¤ºåŠ©æ‰‹å›å¤
            print(f"\nğŸ¤– åŠ©æ‰‹: {assistant_reply}\n")
            
        except Exception as e:
            print(f"\nâŒ é”™è¯¯: {str(e)}\n")
            # ç§»é™¤æœ€åæ·»åŠ çš„ç”¨æˆ·æ¶ˆæ¯
            if messages[-1]["role"] == "user":
                messages.pop()


async def main():
    """ä¸»å‡½æ•°"""
    await chat_with_tools()


if __name__ == "__main__":
    asyncio.run(main())
