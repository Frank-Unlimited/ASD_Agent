"""
èŠå¤©æœåŠ¡ - é›†æˆ LLM å’Œ Tools çš„æ™ºèƒ½åŠ©æ‰‹
"""
from typing import Dict, Any, List, Optional, AsyncGenerator
import json

from .chat_tools import ChatTools, TOOL_DEFINITIONS
from services.LLM_Service.service import get_llm_service


class ChatService:
    """èŠå¤©æœåŠ¡"""
    
    def __init__(self, chat_tools: ChatTools):
        self.chat_tools = chat_tools
        self.llm_service = get_llm_service()
        
        # Tool åç§°åˆ°æ–¹æ³•çš„æ˜ å°„
        self.tool_methods = {
            "record_behavior": self.chat_tools.record_behavior,
            "recommend_game": self.chat_tools.recommend_game,
            "get_child_profile": self.chat_tools.get_child_profile,
            "query_behaviors": self.chat_tools.query_behaviors,
            "query_interests": self.chat_tools.query_interests,
            "query_dimension_progress": self.chat_tools.query_dimension_progress,
            "get_latest_assessment": self.chat_tools.get_latest_assessment,
            "get_recent_games": self.chat_tools.get_recent_games,
            "generate_assessment": self.chat_tools.generate_assessment
        }
    
    async def chat(
        self,
        message: str,
        child_id: str,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        å¤„ç†èŠå¤©æ¶ˆæ¯
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            child_id: å½“å‰å­©å­ID
            conversation_history: å¯¹è¯å†å² [{"role": "user/assistant", "content": "..."}]
        
        Returns:
            {
                "response": "åŠ©æ‰‹å›å¤",
                "tool_calls": [...],  # è°ƒç”¨çš„å·¥å…·åˆ—è¡¨
                "conversation_history": [...]  # æ›´æ–°åçš„å¯¹è¯å†å²
            }
        """
        print(f"[ChatService] æ”¶åˆ°æ¶ˆæ¯: {message[:50]}...")
        
        # åˆå§‹åŒ–å¯¹è¯å†å²
        if conversation_history is None:
            conversation_history = []
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = self._build_system_prompt(child_id)
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [{"role": "system", "content": system_prompt}]
        messages.extend(conversation_history)
        messages.append({"role": "user", "content": message})
        
        # ç¬¬ä¸€æ¬¡è°ƒç”¨ LLMï¼ˆå¯èƒ½è¿”å› tool callsï¼‰
        response = await self.llm_service.call_with_tools(
            messages=messages,
            tools=TOOL_DEFINITIONS,
            temperature=0.7,
            max_tokens=2000
        )
        
        assistant_message = response.get("message", {})
        tool_calls = response.get("tool_calls", [])
        
        # å¦‚æœæœ‰ tool callsï¼Œæ‰§è¡Œå®ƒä»¬
        tool_results = []
        if tool_calls:
            print(f"[ChatService] æ‰§è¡Œ {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
            
            for tool_call in tool_calls:
                tool_name = tool_call.get("name")
                tool_args = tool_call.get("arguments", {})
                
                print(f"[ChatService] è°ƒç”¨å·¥å…·: {tool_name}")
                print(f"[ChatService] å‚æ•°: {tool_args}")
                
                # æ‰§è¡Œå·¥å…·
                result = await self._execute_tool(tool_name, tool_args, child_id)
                tool_results.append({
                    "tool_name": tool_name,
                    "result": result
                })
            
            # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯ä¸­ï¼Œå†æ¬¡è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå›å¤
            # æ³¨æ„ï¼štool_calls éœ€è¦è½¬æ¢ä¸º OpenAI æ ¼å¼
            openai_tool_calls = []
            for tc in tool_calls:
                openai_tool_calls.append({
                    "id": tc.get("id"),
                    "type": "function",
                    "function": {
                        "name": tc.get("name"),
                        "arguments": json.dumps(tc.get("arguments", {}), ensure_ascii=False)
                    }
                })
            
            messages.append({
                "role": "assistant",
                "content": assistant_message.get("content", ""),
                "tool_calls": openai_tool_calls
            })
            
            # æ·»åŠ å·¥å…·ç»“æœ
            for i, tool_call in enumerate(tool_calls):
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.get("id", f"call_{i}"),
                    "name": tool_call.get("name"),
                    "content": json.dumps(tool_results[i]["result"], ensure_ascii=False)
                })
            
            # ç¬¬äºŒæ¬¡è°ƒç”¨ LLMï¼Œç”ŸæˆåŸºäºå·¥å…·ç»“æœçš„å›å¤
            final_response = await self.llm_service.call_with_tools(
                messages=messages,
                tools=TOOL_DEFINITIONS,
                temperature=0.7,
                max_tokens=2000
            )
            
            final_message = final_response.get("message", {})
            response_text = final_message.get("content", "")
        else:
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›å›å¤
            response_text = assistant_message.get("content", "")
        
        # æ›´æ–°å¯¹è¯å†å²
        conversation_history.append({"role": "user", "content": message})
        conversation_history.append({"role": "assistant", "content": response_text})
        
        # é™åˆ¶å¯¹è¯å†å²é•¿åº¦ï¼ˆä¿ç•™æœ€è¿‘10è½®ï¼‰
        if len(conversation_history) > 20:
            conversation_history = conversation_history[-20:]
        
        return {
            "response": response_text,
            "tool_calls": tool_results,
            "conversation_history": conversation_history
        }
    
    def _build_system_prompt(self, child_id: str) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ ASD å„¿ç«¥å®¶åº­å¹²é¢„åŠ©æ‰‹ï¼Œå¸®åŠ©å®¶é•¿è®°å½•å­©å­çš„è¡Œä¸ºã€æ¨èæ¸¸æˆã€æŸ¥è¯¢è¯„ä¼°ç»“æœã€‚

**å½“å‰å­©å­ID**: {child_id}

**æ ¸å¿ƒè§„åˆ™ - å¿…é¡»éµå®ˆ**ï¼š

1. ã€è‡ªåŠ¨è®°å½•ã€‘å½“å®¶é•¿æè¿°å­©å­çš„ä»»ä½•è¡Œä¸ºã€å…´è¶£ã€è¡¨ç°æ—¶ï¼Œä½ å¿…é¡»ç«‹å³è°ƒç”¨ record_behavior å·¥å…·
   - è§¦å‘è¯ï¼šå–œæ¬¢ã€è®¨åŒã€åšäº†ã€è¡¨ç°ã€æ„Ÿå…´è¶£ã€ç©äº†ã€è¯´äº†ã€çœ‹äº†ã€å¬äº†ç­‰
   - ç¤ºä¾‹ï¼š"å­©å­å–œæ¬¢XX" â†’ ç«‹å³è°ƒç”¨ record_behavior
   - ä¸è¦è¯¢é—®"è¦ä¸è¦è®°å½•"ï¼Œç›´æ¥è®°å½•

2. ã€æ•°æ®æŸ¥è¯¢ã€‘å½“å®¶é•¿è¯¢é—®å­©å­çš„æƒ…å†µæ—¶ï¼Œä¸»åŠ¨è°ƒç”¨ç›¸åº”çš„æŸ¥è¯¢å·¥å…·ï¼š
   - "å­©å­ä¹‹å‰åšäº†ä»€ä¹ˆ" â†’ query_behaviors
   - "å­©å­å–œæ¬¢ä»€ä¹ˆ" â†’ query_interests
   - "å­©å­è¿›æ­¥å¦‚ä½•" â†’ query_dimension_progress
   - "æœ€è¿‘ç©äº†ä»€ä¹ˆæ¸¸æˆ" â†’ get_recent_games
   - "ä¸Šæ¬¡è¯„ä¼°ç»“æœ" â†’ get_latest_assessment

3. ã€æ¸¸æˆæ¨èã€‘å½“å®¶é•¿è¯¢é—®"ç©ä»€ä¹ˆ"ã€"æ¨èæ¸¸æˆ"æ—¶ï¼Œè°ƒç”¨ recommend_game

4. ã€è¯„ä¼°ç”Ÿæˆã€‘å½“å®¶é•¿è¦æ±‚"è¯„ä¼°å­©å­"ã€"åšä¸ªè¯„ä¼°"æ—¶ï¼Œè°ƒç”¨ generate_assessmentï¼ˆä¸æ˜¯ get_latest_assessmentï¼‰

**å·¥å…·è°ƒç”¨åçš„å›å¤**ï¼š
- è®°å½•è¡Œä¸ºåï¼šç®€çŸ­ç¡®è®¤ï¼Œå¦‚"å·²è®°å½•âœ“"
- æŸ¥è¯¢æ•°æ®åï¼šç”¨è‡ªç„¶è¯­è¨€æ€»ç»“å…³é”®ä¿¡æ¯ï¼Œä¸è¦ç›´æ¥å¤è¿° JSON
- æ¨èæ¸¸æˆåï¼šç®€è¦ä»‹ç»æ¸¸æˆäº®ç‚¹
- è¯„ä¼°å®Œæˆåï¼šæ€»ç»“å…³é”®å‘ç°ï¼Œç»™äºˆé¼“åŠ±

**äº¤æµé£æ ¼**ï¼š
- æ¸©æš–ã€å‹å¥½ã€ä¸“ä¸š
- ç®€æ´æ˜äº†ï¼Œä¸å•°å—¦
- å…³æ³¨è¿›æ­¥ï¼Œç»™äºˆé¼“åŠ±
- ä¸»åŠ¨ä½¿ç”¨å·¥å…·è·å–æ•°æ®ï¼Œè€Œä¸æ˜¯è¯´"æˆ‘ä¸çŸ¥é“"

**é‡è¦**ï¼š
- è¯†åˆ«åˆ°è¡Œä¸ºæè¿°æ—¶ï¼Œå¿…é¡»ç«‹å³è°ƒç”¨ record_behavior
- é‡åˆ°é—®é¢˜æ—¶ï¼Œä¸»åŠ¨è°ƒç”¨æŸ¥è¯¢å·¥å…·è·å–æ•°æ®
- åŸºäºæ•°æ®å›ç­”é—®é¢˜ï¼Œè€Œä¸æ˜¯çŒœæµ‹
"""
    
    async def _execute_tool(
        self,
        tool_name: str,
        tool_args: Dict[str, Any],
        child_id: str
    ) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        try:
            # ç¡®ä¿ child_id å­˜åœ¨
            if "child_id" not in tool_args:
                tool_args["child_id"] = child_id
            
            # ç›´æ¥è°ƒç”¨å·¥å…·æ–¹æ³•
            tool_method = self.tool_methods.get(tool_name)
            
            if not tool_method:
                return {
                    "success": False,
                    "error": f"æœªçŸ¥çš„å·¥å…·: {tool_name}"
                }
            
            # å¯¼å…¥å‚æ•°ç±»
            from .chat_tools import (
                RecordBehaviorParams,
                RecommendGameParams,
                GetChildProfileParams,
                QueryBehaviorsParams,
                QueryInterestsParams,
                QueryDimensionProgressParams,
                GetLatestAssessmentParams,
                GetRecentGamesParams,
                GenerateAssessmentParams
            )
            
            # å‚æ•°ç±»æ˜ å°„
            param_classes = {
                "record_behavior": RecordBehaviorParams,
                "recommend_game": RecommendGameParams,
                "get_child_profile": GetChildProfileParams,
                "query_behaviors": QueryBehaviorsParams,
                "query_interests": QueryInterestsParams,
                "query_dimension_progress": QueryDimensionProgressParams,
                "get_latest_assessment": GetLatestAssessmentParams,
                "get_recent_games": GetRecentGamesParams,
                "generate_assessment": GenerateAssessmentParams
            }
            
            param_class = param_classes.get(tool_name)
            if not param_class:
                return {
                    "success": False,
                    "error": f"æœªçŸ¥çš„å·¥å…·: {tool_name}"
                }
            
            # éªŒè¯å‚æ•°
            params = param_class(**tool_args)
            
            # æ‰§è¡Œå·¥å…·
            result = await tool_method(params)
            
            return result
        
        except Exception as e:
            print(f"[ChatService] å·¥å…·æ‰§è¡Œå¤±è´¥: {tool_name}, é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                "success": False,
                "error": str(e)
            }
    
    async def chat_stream(
        self,
        message: str,
        child_id: str,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        æµå¼å¤„ç†èŠå¤©æ¶ˆæ¯
        
        ç”Ÿæˆäº‹ä»¶ï¼š
        - {"type": "tool_call", "data": {"tool_name": "...", "arguments": {...}}}
        - {"type": "tool_result", "data": {"tool_name": "...", "result": {...}}}
        - {"type": "content", "data": {"text": "..."}}
        - {"type": "done", "data": {"conversation_history": [...]}}
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            child_id: å½“å‰å­©å­ID
            conversation_history: å¯¹è¯å†å²
        
        Yields:
            äº‹ä»¶å­—å…¸
        """
        print(f"[ChatService Stream] æ”¶åˆ°æ¶ˆæ¯: {message[:50]}...")
        
        # åˆå§‹åŒ–å¯¹è¯å†å²
        if conversation_history is None:
            conversation_history = []
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = self._build_system_prompt(child_id)
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [{"role": "system", "content": system_prompt}]
        messages.extend(conversation_history)
        messages.append({"role": "user", "content": message})
        
        # ç¬¬ä¸€æ¬¡è°ƒç”¨ LLMï¼ˆæµå¼ï¼Œæ£€æµ‹æ˜¯å¦æœ‰ tool callsï¼‰
        tool_calls_data = []
        content_chunks = []
        has_tool_calls = False
        
        async for chunk in self.llm_service.call_with_tools_stream(
            messages=messages,
            tools=TOOL_DEFINITIONS,
            temperature=0.7,
            max_tokens=2000
        ):
            delta = chunk.choices[0].delta if chunk.choices else None
            if not delta:
                continue
            
            # æ”¶é›†å†…å®¹
            if delta.content:
                content_chunks.append(delta.content)
            
            # æ”¶é›†å·¥å…·è°ƒç”¨
            if delta.tool_calls:
                has_tool_calls = True
                for tc in delta.tool_calls:
                    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç©ºé—´
                    while len(tool_calls_data) <= tc.index:
                        tool_calls_data.append({
                            "id": None,
                            "name": None,
                            "arguments": ""
                        })
                    
                    if tc.id:
                        tool_calls_data[tc.index]["id"] = tc.id
                    if tc.function and tc.function.name:
                        tool_calls_data[tc.index]["name"] = tc.function.name
                    if tc.function and tc.function.arguments:
                        tool_calls_data[tc.index]["arguments"] += tc.function.arguments
        
        # æ‰“å°ç¬¬ä¸€æ¬¡ LLM è°ƒç”¨çš„ç»“æœ
        print(f"\n[ChatService] ğŸ“‹ ç¬¬ä¸€æ¬¡ LLM è°ƒç”¨ç»“æœ:")
        print(f"  has_tool_calls: {has_tool_calls}")
        print(f"  tool_calls_data: {json.dumps(tool_calls_data, ensure_ascii=False, indent=2)}")
        print(f"  content_chunks: {content_chunks[:3] if len(content_chunks) > 3 else content_chunks}...")  # åªæ˜¾ç¤ºå‰3ä¸ª
        
        # å¤„ç†å·¥å…·è°ƒç”¨
        tool_results = []
        if has_tool_calls and tool_calls_data:
            print(f"\n[ChatService] ğŸ”§ æ‰§è¡Œ {len(tool_calls_data)} ä¸ªå·¥å…·")
            
            for tool_call in tool_calls_data:
                tool_name = tool_call["name"]
                try:
                    tool_args = json.loads(tool_call["arguments"])
                except json.JSONDecodeError:
                    tool_args = {}
                
                # å‘é€å·¥å…·è°ƒç”¨äº‹ä»¶
                yield {
                    "type": "tool_call",
                    "data": {
                        "tool_name": tool_name,
                        "tool_display_name": self._get_tool_display_name(tool_name),
                        "arguments": tool_args
                    }
                }
                
                # æ‰§è¡Œå·¥å…·
                result = await self._execute_tool(tool_name, tool_args, child_id)
                tool_results.append({
                    "tool_name": tool_name,
                    "result": result
                })
                
                # æ‰“å°å®Œæ•´çš„å·¥å…·è°ƒç”¨ç»“æœ
                print(f"\n[ChatService] ğŸ“‹ å·¥å…·è°ƒç”¨ç»“æœ:")
                print(f"  å·¥å…·: {tool_name}")
                print(f"  å‚æ•°: {json.dumps(tool_args, ensure_ascii=False, indent=2)}")
                print(f"  ç»“æœ: {json.dumps(result, ensure_ascii=False, indent=2)}")
                
                # å‘é€å·¥å…·ç»“æœäº‹ä»¶
                yield {
                    "type": "tool_result",
                    "data": {
                        "tool_name": tool_name,
                        "tool_display_name": self._get_tool_display_name(tool_name),
                        "success": result.get("success", False),
                        "message": result.get("message", "") if result.get("success") else result.get("error", "")
                    }
                }
            
            # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯ä¸­ï¼Œå†æ¬¡è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå›å¤
            openai_tool_calls = []
            for i, tc in enumerate(tool_calls_data):
                openai_tool_calls.append({
                    "id": tc["id"],
                    "type": "function",
                    "function": {
                        "name": tc["name"],
                        "arguments": tc["arguments"]
                    }
                })
            
            messages.append({
                "role": "assistant",
                "content": "".join(content_chunks) if content_chunks else "",
                "tool_calls": openai_tool_calls
            })
            
            # æ·»åŠ å·¥å…·ç»“æœ
            for i, tool_call in enumerate(tool_calls_data):
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call["id"],
                    "name": tool_call["name"],
                    "content": json.dumps(tool_results[i]["result"], ensure_ascii=False)
                })
            
            # ç¬¬äºŒæ¬¡æµå¼è°ƒç”¨ LLMï¼Œç”ŸæˆåŸºäºå·¥å…·ç»“æœçš„å›å¤
            response_text = ""
            async for chunk in self.llm_service.call_with_tools_stream(
                messages=messages,
                tools=TOOL_DEFINITIONS,
                temperature=0.7,
                max_tokens=2000
            ):
                delta = chunk.choices[0].delta if chunk.choices else None
                if delta and delta.content:
                    response_text += delta.content
                    # å®æ—¶å‘é€å†…å®¹
                    yield {
                        "type": "content",
                        "data": {"text": delta.content}
                    }
        else:
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œéœ€è¦é‡æ–°æµå¼è°ƒç”¨ä»¥å®æ—¶è¿”å›å†…å®¹
            # å› ä¸ºç¬¬ä¸€æ¬¡è°ƒç”¨åªæ˜¯æ”¶é›†äº† chunksï¼Œæ²¡æœ‰å®æ—¶å‘é€
            print(f"\n[ChatService] â„¹ï¸  æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œé‡æ–°æµå¼è°ƒç”¨ LLM")
            response_text = ""
            async for chunk in self.llm_service.call_with_tools_stream(
                messages=messages,
                tools=TOOL_DEFINITIONS,
                temperature=0.7,
                max_tokens=2000
            ):
                delta = chunk.choices[0].delta if chunk.choices else None
                if delta and delta.content:
                    response_text += delta.content
                    # å®æ—¶å‘é€å†…å®¹
                    yield {
                        "type": "content",
                        "data": {"text": delta.content}
                    }
        
        # æ›´æ–°å¯¹è¯å†å²
        conversation_history.append({"role": "user", "content": message})
        conversation_history.append({"role": "assistant", "content": response_text})
        
        # é™åˆ¶å¯¹è¯å†å²é•¿åº¦ï¼ˆä¿ç•™æœ€è¿‘10è½®ï¼‰
        if len(conversation_history) > 20:
            conversation_history = conversation_history[-20:]
        
        # å‘é€å®Œæˆäº‹ä»¶
        yield {
            "type": "done",
            "data": {
                "conversation_history": conversation_history,
                "tool_calls": tool_results
            }
        }
    
    def _get_tool_display_name(self, tool_name: str) -> str:
        """è·å–å·¥å…·æ˜¾ç¤ºåç§°"""
        names = {
            'record_behavior': 'è®°å½•è¡Œä¸º',
            'recommend_game': 'æ¨èæ¸¸æˆ',
            'get_child_profile': 'æŸ¥è¯¢æ¡£æ¡ˆ',
            'query_behaviors': 'æŸ¥è¯¢è¡Œä¸ºè®°å½•',
            'query_interests': 'æŸ¥è¯¢å…´è¶£ç‚¹',
            'query_dimension_progress': 'æŸ¥è¯¢ç»´åº¦è¿›å±•',
            'get_latest_assessment': 'æŸ¥è¯¢è¯„ä¼°',
            'get_recent_games': 'æŸ¥è¯¢æ¸¸æˆå†å²',
            'generate_assessment': 'ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š'
        }
        return names.get(tool_name, tool_name)
